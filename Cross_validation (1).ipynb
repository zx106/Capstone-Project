{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, roc_auc_score"
      ],
      "metadata": {
        "id": "zHNjl2ObfDJ8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/drive')\n",
        "# Read the data file\n",
        "data = pd.read_csv('/drive/MyDrive/final.II.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "gk3o336gP_JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50dae24e-4075-4374-8e3d-b07dc5ca5c53"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model without interaction for saturated model\n",
        "model_without_interaction <- glm(Last_Financing_Deal_Type ~State_incorporation+Type_entity+TRADED+five+thirteen+Num_of_name+fame_included, family = binomial(), combine)"
      ],
      "metadata": {
        "id": "rLmoiv8CTkp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the predictors you want to include in the model\n",
        "predictors = [\"State_incorporation\", \"Type_entity\", \"TRADED\", \"five\", \"thirteen\", \"Num_of_name\"]\n",
        "\n",
        "# Extract the predictors and outcome variable from the DataFrame\n",
        "X = data[predictors]\n",
        "y = data['Last_Financing_Deal_Type']\n",
        "\n",
        "# Create the logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Perform cross-validation\n",
        "scoring = {'accuracy': make_scorer(accuracy_score),\n",
        "           'precision': make_scorer(precision_score, average='weighted'),\n",
        "           'recall': make_scorer(recall_score, average='weighted'),\n",
        "           'roc_auc': make_scorer(roc_auc_score, average='weighted')}\n",
        "\n",
        "cv_results = cross_validate(model, X, y, cv=10, scoring=scoring)\n",
        "\n",
        "# Calculate the average performance scores\n",
        "avg_scores = {}\n",
        "for metric in scoring:\n",
        "    avg_scores[metric] = cv_results[f'test_{metric}'].mean()\n",
        "\n",
        "# Calculate the specificity and misclassification rate\n",
        "specificity = 1 - avg_scores['recall']\n",
        "misclassification_rate = 1 - avg_scores['accuracy']\n",
        "\n",
        "# Create a DataFrame to store the average scores\n",
        "results = pd.DataFrame(avg_scores, index=['Average'])\n",
        "\n",
        "# Add specificity and misclassification rate to the results DataFrame\n",
        "results['Specificity'] = specificity\n",
        "results['Misclassification Rate'] = misclassification_rate\n",
        "\n",
        "# Print the results table\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "to3u5MwyTg7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{r LRT model all}\n",
        "State_incorporation+Type_entity+TRADED+five+thirteen+Num_of_name+fame_included+five*fame_included+Num_of_name*five+Num_of_name*thirteen+five*State_incorporation+thirteen*State_incorporation"
      ],
      "metadata": {
        "id": "d8x6-JBIUbmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the predictors you want to include in the model\n",
        "X = data[[\"State_incorporation\", \"Type_entity\", \"TRADED\", \"five\", \"thirteen\", \"Num_of_name\"]]\n",
        "y = data['Last_Financing_Deal_Type']\n",
        "# Add the interaction term to the predictors\n",
        "\n",
        "X['interaction1'] = X['thirteen'] * X['State_incorporation']\n",
        "X['interaction2'] = X['Num_of_name'] * X['five']\n",
        "X['interaction3'] = X['Num_of_name'] * X['thirteen']\n",
        "X['interaction4'] = X['five'] * X['State_incorporation']\n",
        "\n",
        "\n",
        "# Create the logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Perform cross-validation\n",
        "scoring = {'accuracy': make_scorer(accuracy_score),\n",
        "           'precision': make_scorer(precision_score, average='weighted'),\n",
        "           'recall': make_scorer(recall_score, average='weighted'),\n",
        "           'roc_auc': make_scorer(roc_auc_score, average='weighted')}\n",
        "\n",
        "cv_results = cross_validate(model, X, y, cv=10, scoring=scoring)\n",
        "\n",
        "# Calculate the average performance scores\n",
        "avg_scores = {}\n",
        "for metric in scoring:\n",
        "    avg_scores[metric] = cv_results[f'test_{metric}'].mean()\n",
        "\n",
        "# Calculate the specificity and misclassification rate\n",
        "specificity = 1 - avg_scores['recall']\n",
        "misclassification_rate = 1 - avg_scores['accuracy']\n",
        "\n",
        "# Create a DataFrame to store the average scores\n",
        "results = pd.DataFrame(avg_scores, index=['Average'])\n",
        "\n",
        "# Add specificity and misclassification rate to the results DataFrame\n",
        "results['Specificity'] = specificity\n",
        "results['Misclassification Rate'] = misclassification_rate\n",
        "\n",
        "# Print the results table\n",
        "print(results)\n",
        "\n"
      ],
      "metadata": {
        "id": "7Hl1Gs6WUmeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{r LRT model with most important predictors and all interaction terms}\n",
        "five+thirteen+Num_of_name+fame_included+five*fame_included+Num_of_name*five+Num_of_name*thirteen"
      ],
      "metadata": {
        "id": "-DJmrE17V67h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the predictors you want to include in the model\n",
        "X = data[[\"five\", \"thirteen\", \"Num_of_name\"]]\n",
        "y = data['Last_Financing_Deal_Type']\n",
        "# Add the interaction term to the predictors\n",
        "\n",
        "\n",
        "X['interaction1'] = X['Num_of_name'] * X['five']\n",
        "X['interaction2'] = X['Num_of_name'] * X['thirteen']\n",
        "\n",
        "\n",
        "# Create the logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Perform cross-validation\n",
        "scoring = {'accuracy': make_scorer(accuracy_score),\n",
        "           'precision': make_scorer(precision_score, average='weighted'),\n",
        "           'recall': make_scorer(recall_score, average='weighted'),\n",
        "           'roc_auc': make_scorer(roc_auc_score, average='weighted')}\n",
        "\n",
        "cv_results = cross_validate(model, X, y, cv=10, scoring=scoring)\n",
        "\n",
        "# Calculate the average performance scores\n",
        "avg_scores = {}\n",
        "for metric in scoring:\n",
        "    avg_scores[metric] = cv_results[f'test_{metric}'].mean()\n",
        "\n",
        "# Calculate the specificity and misclassification rate\n",
        "specificity = 1 - avg_scores['recall']\n",
        "misclassification_rate = 1 - avg_scores['accuracy']\n",
        "\n",
        "# Create a DataFrame to store the average scores\n",
        "results = pd.DataFrame(avg_scores, index=['Average'])\n",
        "\n",
        "# Add specificity and misclassification rate to the results DataFrame\n",
        "results['Specificity'] = specificity\n",
        "results['Misclassification Rate'] = misclassification_rate\n",
        "\n",
        "# Print the results table\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "gF32hevLWEz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{r LRT model with most important predictors without interaction terms}\n",
        "five+thirteen+Num_of_name+fame_included"
      ],
      "metadata": {
        "id": "OwaiMu_TW1Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Specify the predictors you want to include in the model\n",
        "predictors = ['five', 'thirteen', 'Num_of_name']\n",
        "\n",
        "# Extract the predictors and outcome variable from the DataFrame\n",
        "X = data[predictors]\n",
        "y = data['Last_Financing_Deal_Type']\n",
        "\n",
        "# Create the logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Perform cross-validation\n",
        "scoring = {'accuracy': make_scorer(accuracy_score),\n",
        "           'precision': make_scorer(precision_score, average='weighted'),\n",
        "           'recall': make_scorer(recall_score, average='weighted'),\n",
        "           'roc_auc': make_scorer(roc_auc_score, average='weighted')}\n",
        "\n",
        "cv_results = cross_validate(model, X, y, cv=10, scoring=scoring)\n",
        "\n",
        "# Calculate the average performance scores\n",
        "avg_scores = {}\n",
        "for metric in scoring:\n",
        "    avg_scores[metric] = cv_results[f'test_{metric}'].mean()\n",
        "\n",
        "# Calculate the specificity and misclassification rate\n",
        "specificity = 1 - avg_scores['recall']\n",
        "misclassification_rate = 1 - avg_scores['accuracy']\n",
        "\n",
        "# Create a DataFrame to store the average scores\n",
        "results = pd.DataFrame(avg_scores, index=['Average'])\n",
        "\n",
        "# Add specificity and misclassification rate to the results DataFrame\n",
        "results['Specificity'] = specificity\n",
        "results['Misclassification Rate'] = misclassification_rate\n",
        "\n",
        "# Print the results table\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "YJfJIQHdXDnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AIC Optimization model: five + thirteen + fame_included"
      ],
      "metadata": {
        "id": "eD0hw-mKXe6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the predictors you want to include in the model\n",
        "predictors = ['five', 'thirteen']\n",
        "\n",
        "# Extract the predictors and outcome variable from the DataFrame\n",
        "X = data[predictors]\n",
        "y = data['Last_Financing_Deal_Type']\n",
        "\n",
        "# Create the logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Perform cross-validation\n",
        "scoring = {'accuracy': make_scorer(accuracy_score),\n",
        "           'precision': make_scorer(precision_score, average='weighted'),\n",
        "           'recall': make_scorer(recall_score, average='weighted'),\n",
        "           'roc_auc': make_scorer(roc_auc_score, average='weighted')}\n",
        "\n",
        "cv_results = cross_validate(model, X, y, cv=10, scoring=scoring)\n",
        "\n",
        "# Calculate the average performance scores\n",
        "avg_scores = {}\n",
        "for metric in scoring:\n",
        "    avg_scores[metric] = cv_results[f'test_{metric}'].mean()\n",
        "\n",
        "# Calculate the specificity and misclassification rate\n",
        "specificity = 1 - avg_scores['recall']\n",
        "misclassification_rate = 1 - avg_scores['accuracy']\n",
        "\n",
        "# Create a DataFrame to store the average scores\n",
        "results = pd.DataFrame(avg_scores, index=['Average'])\n",
        "\n",
        "# Add specificity and misclassification rate to the results DataFrame\n",
        "results['Specificity'] = specificity\n",
        "results['Misclassification Rate'] = misclassification_rate\n",
        "\n",
        "# Print the results table\n",
        "print(results)\n",
        "\n"
      ],
      "metadata": {
        "id": "q76OkZ7FXeWm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}